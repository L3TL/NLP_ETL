{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 Report\n",
    "<b> Contains the code, and report as comments </b> <br> \n",
    "\n",
    "The following script presented below is tasked with reading and processing information acquired into something readable, along with the statistics required to make an informed decision regarding it. This report will cover the code blocks performing so, alongsides the reasons behind the results and choice of methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block covers the imports and variables instantiated. There is no real significant mention here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json\n",
    "from nltk import FreqDist, collections\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "from functools import lru_cache\n",
    "from nltk import word_tokenize\n",
    "import math\n",
    "import time\n",
    "\n",
    "# PartA:Q1\n",
    "data = []\n",
    "lmm = nltk.WordNetLemmatizer()\n",
    "lemma_tize = lru_cache(maxsize=50000)(lmm.lemmatize)\n",
    "vocab = Counter()\n",
    "total_length = 0\n",
    "positive_count = 0\n",
    "negative_count = 0\n",
    "news_pos = 0\n",
    "news_neg = 0\n",
    "C0 = 0\n",
    "word_list = []\n",
    "pos_list = set()\n",
    "neg_list = set()\n",
    "gen_sent = ['is','this']\n",
    "wordlist1 = []\n",
    "wordlist2 = []\n",
    "test_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section reads the positive and negative word text files (I can't seem to make it read anything in a subdirectory, so I had to place them in the same directory as this script), saving them as a key to a set: one positive, and one negative.\n",
    "\n",
    "There is also a function for POS tagging that has been commented out, which will be addressed further down during the lemmatization phase\n",
    "\n",
    "There is also another function, regTokenize(), which is another way of tokenizing that I chose purely because it had less overhead and did roughly the same job as nltk.word_tokenize, thus taking less time to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"positive-words.txt\") as p:\n",
    "    for line in p:\n",
    "        key = p.readline()\n",
    "        key = key.strip()\n",
    "        pos_list.add(key)\n",
    "    # print(pos_list)\n",
    "with open(r\"negative-words.txt\") as p:\n",
    "    for line in p:\n",
    "        key = p.readline()\n",
    "        key = key.strip()\n",
    "        neg_list.add(key)\n",
    "    # print(neg_list)\n",
    "    \n",
    "# def get_pos(word):\n",
    "#     w_synsets = wordnet.synsets(word)\n",
    "#     pos_counts = Counter()\n",
    "#     pos_counts[\"n\"] = len([item for item in w_synsets if item.pos() == \"n\"])\n",
    "#     pos_counts[\"v\"] = len([item for item in w_synsets if item.pos() == \"v\"])\n",
    "#     pos_counts[\"a\"] = len([item for item in w_synsets if item.pos() == \"a\"])\n",
    "#     pos_counts[\"r\"] = len([item for item in w_synsets if item.pos() == \"r\"])\n",
    "#     most_common_pos_list = pos_counts.most_common(3)\n",
    "#     return most_common_pos_list[0][0]\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "def regTokenize(text):\n",
    "    words = WORD.findall(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block has a massive pile of code sections mashed into a single for loop, so for the sake of convenience and navigation, comment markers with their respective numbers have been used:\n",
    "\n",
    "[1] This section opens the jsonl file and loads in only the \"content\" element of each line, thus processing just the news stories. It is also where the text preprocessing is done, such that:\n",
    "    * URLS are filtered out such that any word containing HTTP until the first whitespace is removed. I had considered adding another piece of filter to include words with no spacing and a \".\", such as 'abcd.efg'. I assumed this to be dangerous as poor formatting of the file may result in unnecessary deletions*\n",
    "    * Filtering of nonalphanumeric characters, words of 1 length, and pure numbers. standard regex sub process here*\n",
    "The text is then tokenized with a custom function previously mentioned above*\n",
    "\n",
    "[2] This section counts the overall occurences of positive/negative words, as well as an instantiated variable of current positive/negative words, to tally into the news stories which has more positives than negatives, and vice versa. \n",
    "* Not much to be said here, other than a set was used since order/value of the keys did not matter, just its presence*\n",
    "\n",
    "[3] This section tallies the overall tokens per news story, into an overall total count, alongsides updating the dictionary with unique word occurences per story. \n",
    "Lemmatization of the tokenized story was also done here, saved as a string to a list, for future processing\n",
    "* The lemmatization function is a basic wordnetlemmatizer in the initialization phase, but cached for performance. I also did not use the POS tagger function (from above) due to the absurdly long time it takes to process. This does however affect the trigrams and overall wording as some words like 'as', become 'a'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token count: 5773603\n",
      "vocabulary count: 19175\n",
      "total positive: 95412 | total negative: 62333\n",
      "postive>negative:  10512 | negative>positive: 5388\n"
     ]
    }
   ],
   "source": [
    "# start_time = time.time()\n",
    "#[1]=============================================\n",
    "with open('signal-news1.jsonl') as f:\n",
    "    for line in f.readlines():\n",
    "        obj = json.loads(line)\n",
    "        x = obj[\"content\"].lower()\n",
    "        #--------\n",
    "        test_list.append(x)\n",
    "        x = re.sub(r\"http\\S+\", \"\", x)\n",
    "        x = re.sub(r\"\\s*[^\\w=]+\\s*\", \" \", x)\n",
    "        x = re.sub(r\"\\b\\w{1,1}\\b\", \"\", x)\n",
    "        x = re.sub(r'\\b[0-9]+\\b', '', x)\n",
    "        y = regTokenize(x)\n",
    "#[1]---------------------------------------------\n",
    "#[2]=============================================\n",
    "        pos_news_count = 0\n",
    "        neg_news_count = 0\n",
    "        for i in y:\n",
    "            if i in pos_list:\n",
    "                positive_count += 1\n",
    "                pos_news_count += 1\n",
    "            if i in neg_list:\n",
    "                negative_count += 1\n",
    "                neg_news_count += 1\n",
    "        if pos_news_count > neg_news_count:\n",
    "            news_pos += 1\n",
    "        elif pos_news_count < neg_news_count:\n",
    "            news_neg += 1\n",
    "        # print(positive_count, pos_news_count)\n",
    "        # y = word_tokenize(x)\n",
    "#[2]----------------------------------------------\n",
    "#[3]===============================================\n",
    "        total_length += len(y)\n",
    "        y = ' '.join(lemma_tize(word) for word in y)\n",
    "        word_list.append(y)\n",
    "        [vocab.update(s.split()) for s in y]\n",
    "#[3]-----------------------------------------------\n",
    "f.close()\n",
    "print(\"token count:\", total_length)\n",
    "print(\"vocabulary count:\", (len(set(word_list))))\n",
    "print(\"total positive:\",positive_count,\"| total negative:\", negative_count)\n",
    "print(\"postive>negative: \", news_pos, \"| negative>positive:\", news_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] This section splits the aforementioned wordlist, into sections, one up to 16,000 rows, and the other 16,000+.\n",
    "The trigram after was formed by converting the wordlist (corpus) into a giant, single string, before tokenizing and making trigrams of it. The top 25 were displayed after\n",
    "\n",
    "[5] This section runs a similiar version of [4], with the exception that it only does so up to the 16,000 rows. It also has a logic where it considers 2 words, \"filter1\" and \"filter2\" as parameters, to find the most common trigram beginning with those 2. The last 2 words are then set to \"filter1\" and \"filter2\", whilst the last word in the trigram is added to a list. That repeats 8 times (since the list already has 'is' and 'this') to form the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 trigrams:  [(('one', 'of', 'the'), 2440), (('on', 'share', 'of'), 2095), (('day', 'moving', 'average'), 1979), (('on', 'the', 'stock'), 1567), (('a', 'well', 'a'), 1426), (('in', 'research', 'report'), 1417), (('in', 'research', 'note'), 1375), (('the', 'year', 'old'), 1261), (('the', 'united', 'state'), 1227), (('for', 'the', 'quarter'), 1221), (('average', 'price', 'of'), 1193), (('research', 'report', 'on'), 1177), (('research', 'note', 'on'), 1138), (('the', 'end', 'of'), 1134), (('share', 'of', 'the'), 1132), (('in', 'report', 'on'), 1124), (('earnings', 'per', 'share'), 1123), (('buy', 'rating', 'to'), 1075), (('cell', 'phone', 'plan'), 1073), (('phone', 'plan', 'detail'), 1070), (('according', 'to', 'the'), 1069), (('of', 'the', 'company'), 1058), (('appeared', 'first', 'on'), 995), (('moving', 'average', 'price'), 995), (('price', 'target', 'on'), 981)]\n",
      "is this (('is', 'this', 'the'), 5)\n",
      "this the (('this', 'the', 'company'), 4)\n",
      "the company (('the', 'company', 'diesel'), 4)\n",
      "company diesel (('company', 'diesel', 'car'), 2)\n",
      "diesel car (('diesel', 'car', 'they'), 1)\n",
      "car they (('car', 'they', 'could'), 1)\n",
      "they could (('they', 'could', 'develop'), 1)\n",
      "could develop (('could', 'develop', 'atherosclerosis'), 1)\n",
      "generated sentence:  ['is', 'this', 'the', 'company', 'diesel', 'car', 'they', 'could', 'develop', 'atherosclerosis']\n"
     ]
    }
   ],
   "source": [
    "#[4]======================================================\n",
    "word1 = word_list[:16000]\n",
    "word2 = word_list[16001:]\n",
    "test = test_list[16001:]\n",
    "\n",
    "z = [' '.join(sentence.split()) for sentence in word_list]\n",
    "joined = \" \".join(z)\n",
    "trigrams = nltk.trigrams(nltk.word_tokenize(joined))\n",
    "fd = Counter(trigrams)\n",
    "print(\"Top 25 trigrams: \",fd.most_common(25))\n",
    "#[4]--------------------------------------------------------\n",
    "#[5]=========================================================\n",
    "z1 = [' '.join(sentence.split()) for sentence in word1]\n",
    "joined1 = \" \".join(z1)\n",
    "tri1 = nltk.trigrams(nltk.word_tokenize(joined1))\n",
    "fd1 = Counter(tri1)\n",
    "filter1 = 'is'\n",
    "filter2 = 'this'\n",
    "counter = 0\n",
    "sentencelist = ['is','this']\n",
    "for trigram in fd1.most_common():\n",
    "    if (filter1 == trigram[0][0]) and (filter2 == trigram[0][1]):\n",
    "        #print(filter1, filter2, trigram)\n",
    "        sentencelist.append(trigram[0][2])\n",
    "        filter1 = trigram[0][1]\n",
    "        filter2 = trigram[0][2]\n",
    "        counter += 1\n",
    "    if counter == 8:\n",
    "        break\n",
    "print(\"generated sentence: \", sentencelist)\n",
    "#[5]-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6] The final section involves a convoluted sequence, so it has to be explained in this text rather than in the code:\n",
    "   * [6.1] The remaining rows in the corpus began with a fresh, unprocessed list from row 16,000 onwards. The reason is because having the processed/lemmatized version would remove all stopwords, thus making it impossible to distinguish the sentences per news story. Each line (sentence) in sentence (news story) is then saved to an array, named 'arr'. There are 2 ngrams used here: a bigram and unigram (to form the [[x,y],[z]] trigram model, being the probability of z given x,y)\n",
    "   * [6.2] Each sentence in the array is now having their probability of occuring in that sequence, calculated, using the method in [6.3]. This will be summed as the final overall perplexity\n",
    "   * [6.3] This function checks each word in that sentence, then calculates the probability of all of them occuring (By saving the value from [6.4], given the occurance of a word, into a list, then summing the value, before returning it in the chain rule formula)\n",
    "   * [6.4] This function calculates the probability of a unigram following a bigram (I realized computing the probability using a trigram is too complex to code, so I went with a bigram calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 16001+ perplexity: 107.98467530302058\n"
     ]
    }
   ],
   "source": [
    "#[6]=============================================================\n",
    "#[6.4]~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "def bigramprobability(bigram, unigram_list, bigram_list):\n",
    "    word1 = str(bigram[0])\n",
    "    word2 = str(bigram[1])\n",
    "    sent1 = bigram_list.get((word1, word2))\n",
    "    sent2 = unigram_list.get((word1,))\n",
    "    return sent1/sent2\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#[6.1]~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "tokenizedcorpus = [nltk.sent_tokenize(content) for content in test]\n",
    "arr = []\n",
    "for sentence in tokenizedcorpus:\n",
    "    for line in sentence:\n",
    "        arr.append(line)\n",
    "bigram_list = Counter()\n",
    "unigram_list =Counter()\n",
    "for line in arr:\n",
    "    y = nltk.word_tokenize(line)\n",
    "    bigram_list.update(nltk.ngrams(y,2))\n",
    "    unigram_list.update(nltk.ngrams(y,1))\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#[6.3]~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "def sentence_probability(sentence, bigram_list, unigram_list):\n",
    "    val = []\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    sentlength = len(words)\n",
    "    finalval = 1\n",
    "    for i in range(1,sentlength):\n",
    "        bigrams = [words[i-1],words[i]]\n",
    "        prob = bigramprobability(bigrams,unigram_list,bigram_list)\n",
    "        # print(prob)\n",
    "        value = 1/prob\n",
    "        val.append(value)\n",
    "    for x in val:\n",
    "        finalval *= x\n",
    "    # print(finalval)\n",
    "    return 2**-((1/sentlength) * finalval)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#[6.2]~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "totalvalue=0\n",
    "for sentence in arr:\n",
    "    sentprob = sentence_probability(sentence, bigram_list, unigram_list)\n",
    "    # print(sentprob)\n",
    "    totalvalue += sentprob\n",
    "    # print(totalvalue)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "print(\"row 16001+ perplexity:\", totalvalue)\n",
    "#[6]------------------------------------------------------\n",
    "#-------------------------------------------\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
