# NLP_ETL
When it comes to understanding data, one of the major issues that occur is in processing and handling the sheer volume of information that comes off the internet.
This jupyter notebook is meant to detail the steps taken from extracting and handling a JSON corpus spanning 19,000+ entries, into a nltk model that calculates the perplexity of trigrams, starting from the bigram "is, this"

Note that the actual JSON itself has been shrunk to fit Github's size limitation, down to 10,000 from 19,228. The purpose of the file is to showcase the idea of what type of data is being worked with
